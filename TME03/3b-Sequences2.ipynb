{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c1a674",
   "metadata": {},
   "source": [
    "**Ben KABONGO**, *21116436*, M1 DAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279abe5",
   "metadata": {},
   "source": [
    "# Word Embedding for Sequence Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879c909",
   "metadata": {},
   "source": [
    "**The goal of this practical is to use pre-trained word embedding for adressing the sequence prediction tasks studied in week 2: PoS and chunking.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdc2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527dba92",
   "metadata": {},
   "source": [
    "## 0) Loading PoS (or chunking) datasets (small or large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1478369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    listeDoc = list()\n",
    "    with open(filename, \"r\") as f:\n",
    "        doc = list()\n",
    "        for ligne in f:\n",
    "            #print \"l : \",len(ligne),\" \",ligne\n",
    "            if len(ligne) < 2: # fin de doc\n",
    "                listeDoc.append(doc)\n",
    "                doc = list()\n",
    "                continue\n",
    "            mots = ligne.replace(\"\\n\",\"\").split(\" \")\n",
    "            doc.append((mots[0],mots[2])) # mettre mots[2] Ã  la place de mots[1] pour le chuncking\n",
    "    return listeDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0514890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8936  docs read\n",
      "2012  docs (T) read\n"
     ]
    }
   ],
   "source": [
    "bSmall = False\n",
    "directory = \"../TME02/ressources/conll2000/\"\n",
    "\n",
    "if(bSmall==True):\n",
    "    filename = \"chtrain.txt\" \n",
    "    filenameT = \"chtest.txt\" \n",
    "\n",
    "else:\n",
    "    # Larger corpus .\n",
    "    filename = \"train.txt\" \n",
    "    filenameT = \"test.txt\" \n",
    "\n",
    "alldocs = load(directory + filename)\n",
    "alldocsT = load(directory + filenameT)\n",
    "\n",
    "print(len(alldocs),\" docs read\")\n",
    "print(len(alldocsT),\" docs (T) read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a10ae1",
   "metadata": {},
   "source": [
    "# 1) Word embedding for classifying each word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91fa49d",
   "metadata": {},
   "source": [
    "### Pre-trained word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9688afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "bload = True\n",
    "fname = \"word2vec-google-news-300\"\n",
    "sdir = \"\" # Change\n",
    "\n",
    "if(bload==True):\n",
    "    wv_pre_trained = KeyedVectors.load(sdir+fname+\".dat\")\n",
    "else:    \n",
    "    wv_pre_trained = api.load(fname)\n",
    "    wv_pre_trained.save(sdir+fname+\".dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9dc19",
   "metadata": {},
   "source": [
    "### Some token on the dataset are missing, we will encode them with a random vector\n",
    "This is sub-optimal, but we need to do something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38abc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomvec():\n",
    "    default = np.random.randn(300)\n",
    "    default = default  / np.linalg.norm(default)\n",
    "    return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd9a228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random vectors : 3576\n",
      "Random vectors test : 659\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=10) # seed the randomness\n",
    "\n",
    "dictadd = dict()\n",
    "cpt=0\n",
    "i = 0\n",
    "for d in alldocs:\n",
    "    cpt+=1\n",
    "    #print(\" ****** Document ******\",cpt)\n",
    "    for (x,pos) in d:\n",
    "        if (not (x in wv_pre_trained) and not (x in dictadd)):\n",
    "            #print(x,\" not in WE, adding it with random vector\")\n",
    "            i += 1\n",
    "            dictadd[x] = randomvec()\n",
    "print('Random vectors :', i)\n",
    "    \n",
    "\n",
    "i = 0\n",
    "for d in alldocsT:\n",
    "    cpt+=1\n",
    "    #print(\" ****** TEST Document ******\",cpt)\n",
    "    for (x,pos) in d:\n",
    "        if (not (x in wv_pre_trained) and not (x in dictadd)):\n",
    "            #print(x,\" not in WE, adding it with random vector\")\n",
    "            i += 1\n",
    "            dictadd[x] = randomvec()\n",
    "print('Random vectors test :', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94cd47",
   "metadata": {},
   "source": [
    "### Add the (key-value) 'random' word embeddings for missing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b202e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_pre_trained.add_vectors(list(dictadd.keys()), list(dictadd.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cb9f7",
   "metadata": {},
   "source": [
    "### Store the train and test datasets: a word embedding for each token in the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e1f1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "wvectors = [wv_pre_trained[word] for doc in alldocs for word, tag in doc]\n",
    "wvectorsT = [wv_pre_trained[word] for doc in alldocsT for word, tag in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be97535",
   "metadata": {},
   "source": [
    "### Check the size of your train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e9a561b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211727, 47377)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wvectors), len(wvectorsT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ca328",
   "metadata": {},
   "source": [
    "### Collecting train/test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e2b7173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22  keys in the dictionary\n",
      "23  keys in the dictionary\n"
     ]
    }
   ],
   "source": [
    "# Labels train/test\n",
    "\n",
    "buf2 = [[pos for m,pos in d ] for d in alldocs]\n",
    "cles = []\n",
    "[cles.extend(b) for b in buf2]\n",
    "cles = np.unique(np.array(cles))\n",
    "cles2ind = dict(zip(cles,range(len(cles))))\n",
    "nCles = len(cles)\n",
    "print(nCles,\" keys in the dictionary\")\n",
    "\n",
    "labels  = np.array([cles2ind[pos] for d in alldocs for (m,pos) in d ])\n",
    "#np.array([cles2ind[pos] for (m,pos) in d for d in alldocs])\n",
    "labelsT  = np.array([cles2ind.setdefault(pos,len(cles)) for d in alldocsT for (m,pos) in d ])\n",
    "\n",
    "print(len(cles2ind),\" keys in the dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "001760be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211727,)\n",
      "(47377,)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "print(labelsT.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5133930",
   "metadata": {},
   "source": [
    "### Train a Logistic Regression Model! \n",
    "**An compare performances to the baseline and sequence models (HMM/CRF) or practical 2a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1c94ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benkabongo25/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7718724275492328"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(wvectors, labels)\n",
    "\n",
    "labelsP = lr.predict(wvectorsT)\n",
    "accuracy_score(labelsP, labelsT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbd43a",
   "metadata": {},
   "source": [
    "# 2) Using word embedding with CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6ce24",
   "metadata": {},
   "source": [
    "## We will define the following features functions for CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3668c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_wv(sentence, index):\n",
    "    v = wv_pre_trained.get_vector(sentence[index])\n",
    "    d = {'f'+str(i):v[i] for i in range(300)}\n",
    "    return d\n",
    "\n",
    "def features_structural(sentence, index):\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "     ## We will define the following features functions for CRF## We will define the following features functions for CRF   'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "def features_wv_plus_structural(sentence, index):\n",
    "    v = wv_pre_trained.get_vector(sentence[index]) \n",
    "    d = {'f'+str(i):v[i] for i in range(300)}\n",
    "\n",
    "    return {**d, **features_structural(sentence, index)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38a9f9",
   "metadata": {},
   "source": [
    "## [Question]: explain what the 3 feature functions encode and what their differences are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88291f28",
   "metadata": {},
   "source": [
    "`\n",
    "The features_wv function encodes word embeddings for a given word in a sentence. It uses a pre-trained word embedding model (in this case, wv_pre_trained) to retrieve a vector representation for the word. The function creates a dictionary with keys f0 to f299, where each key represents a dimension of the word embedding vector. This function captures the semantic similarity between words, since words with similar meanings tend to have similar vector representations in the embedding space.\n",
    "`\n",
    "\n",
    "`\n",
    "The features_structural function encodes structural features of a given word in a sentence. It creates a dictionary with keys that encode various aspects of the word, such as its position in the sentence, capitalization, prefixes and suffixes, adjacent words, and other properties like presence of hyphens or numeric digits. This function captures the syntactic and morphological properties of the words in the sentence.\n",
    "`\n",
    "\n",
    "`\n",
    "The features_wv_plus_structural function combines the word embedding and structural features by concatenating their respective dictionaries. It creates a new dictionary with keys that represent both the dimensions of the word embedding vector and the structural features of the word. This function captures both the semantic and syntactic/morphological properties of the words in the sentence.\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6a31f",
   "metadata": {},
   "source": [
    "### You can now train a CRF with the 3 features and analyse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6b1e451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/mh6g2gbj3bl342zd3jjfff0c0000gn/T/ipykernel_97232/3516758366.py:8: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  tagger.evaluate(alldocsT)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8817147561052832"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.crf import CRFTagger\n",
    "\n",
    "tagger = CRFTagger(feature_func=features_wv)\n",
    "## Train the model\n",
    "tagger.train(alldocs, 'model_w2v_crf_1')\n",
    "\n",
    "## Evaluate performances\n",
    "tagger.evaluate(alldocsT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ce503b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/mh6g2gbj3bl342zd3jjfff0c0000gn/T/ipykernel_97232/2102149138.py:6: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  tagger.evaluate(alldocsT)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9384089326044283"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = CRFTagger(feature_func=features_structural)\n",
    "## Train the model\n",
    "tagger.train(alldocs, 'model_w2v_crf_2')\n",
    "\n",
    "## Evaluate performances\n",
    "tagger.evaluate(alldocsT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0585492d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/mh6g2gbj3bl342zd3jjfff0c0000gn/T/ipykernel_97232/3365007237.py:6: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  tagger.evaluate(alldocsT)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9452054794520548"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = CRFTagger(feature_func=features_wv_plus_structural)\n",
    "## Train the model\n",
    "tagger.train(alldocs, 'model_w2v_crf_1')\n",
    "\n",
    "## Evaluate performances\n",
    "tagger.evaluate(alldocsT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
